# -*- coding: utf-8 -*-

"""
æ–‡æœ¬é¢„å¤„ç†çš„matx text pipe
"""
from typing import List, Dict
import numpy
import os

import torch
import matx
import matx_text

from fex.matx.text_ops import BertTokenizer, BertInputsBuilder, \
    MultiDomainConcatBuilder, TaskManager
from fex.utils.hdfs_io import HADOOP_BIN


def get_local_vocab_file(file_path: str):
    if file_path.startswith("hdfs"):
        file_name = os.path.split(file_path)[-1]
        local_file_path = os.path.join(os.path.abspath(
            os.path.dirname(__file__)), file_name)
        if not os.path.exists(local_file_path):
            os.system("{} dfs -get {} {}".format(HADOOP_BIN,
                                                 file_path, local_file_path))
        return local_file_path
    return file_path


class MatxTextPipe:
    """
    matx æ–‡æœ¬å¤„ç† pipelineã€‚
    åŠŸèƒ½ï¼šå°†ä¸€ä¸ªbatchçš„åŽŸå§‹æ•°æ®ï¼Œè½¬æ¢æˆtensorã€‚

    æœ‰ä¸¤ä¸ªmodeï¼Œä¸€ä¸ªæ˜¯training modeï¼Œä¸€ä¸ªæ˜¯trace modeã€‚
    training modeä¸‹ï¼Œinputæ˜¯raw stringï¼Œoutputæ˜¯torch.tensor
    trace mode ä¸‹ï¼Œinputæ˜¯raw stringï¼Œoutputæ˜¯ matx ndarray
    æ ¹æ® `is_trace` å‚æ•°æ¥æŽ§åˆ¶å“ªä¸ªmode
    """

    def __init__(self,
                 vocab_file: str,
                 max_seq_len: int,
                 add_title: bool = False,
                 do_lower_case: bool = False,
                 tokenize_emoji: bool = False,
                 greedy_sharp: bool = True,
                 is_trace: bool = False,
                 thread_num: bool = 4
                 ):
        """
        Matx ç‰ˆæ–‡æœ¬ Pipeline
        Args:
            vocab_file: è¯è¡¨
            max_seq_len: æœ€å¤§é•¿åº¦
            image_token_num: å›¾ç‰‡ tokençš„æ•°é‡ï¼Œä¸»è¦æ˜¯ç”¨äºŽå¤„ç† image embedding çš„paddingé€»è¾‘
            image_feature_dim: å›¾ç‰‡ embedding çš„dimensionï¼Œä½œç”¨åŒä¸Š
            do_lower_case: tokenzier çš„é…ç½®ï¼Œæ˜¯å¦å…¨å°å†™
            tokenize_emoji: tokenzier çš„é…ç½®ï¼Œæ˜¯å¦ç»™ emoji ä¸¤è¾¹éƒ½åŠ ç©ºæ ¼ï¼Œä½œä¸ºä¸€ä¸ªå•ç‹¬çš„è¯çœ‹å¾…ã€‚
                ä¸€èˆ¬åœ¨å…¶ä»–è¯­ç§ç”Ÿæ•ˆï¼Œä¸­æ–‡åˆ†è¯libcuté»˜è®¤æˆ–å°†emojiåˆ†å¼€ã€‚å¤–è¯­å¦‚è‹±è¯­æ²¡æœ‰åˆ†è¯çš„è¿‡ç¨‹ï¼Œemojiå®¹æ˜“é€ æˆUNKã€‚
                å¦‚ "happyðŸ˜º"ï¼Œéœ€è¦è¯è¡¨é‡ŒåŒ…å« "##ðŸ˜º"ï¼Œå¦åˆ™ä¼šUNKã€‚è¿™ä¸ªå¼€å…³åœ¨ä¸­æ–‡ä¸‹åŸºæœ¬æ— ç”¨ï¼Œä¸»è¦æ˜¯è‹±è¯­ç”¨
            greedy_sharp: tokenizer çš„é…ç½®ï¼Œæ˜¯å¦è´ªå¿ƒçš„åŒ¹é… æ— ##çš„è¯ã€‚
                å¦‚æžœgreedy_sharpä¸ºtrueï¼Œå³ä½¿ç”¨æ— ##æ¨¡å¼ï¼Œå¦‚æžœgreedy_sharpä¸ºfalseï¼Œå³ä½¿ç”¨æœ‰##æ¨¡å¼ã€‚ç­‰ä»·äºŽ not `do_wordpiece`ã€‚
                é»˜è®¤ä¸ºtrueã€‚
                åœ¨bpeçš„æ—¶å€™ï¼Œå¯¹è¢«åˆ‡å¼€çš„è¯çš„åŽåŠéƒ¨åˆ†ï¼Œå¿…é¡»å¾—æ˜¯ "##x" çš„å½¢å¼ï¼Œè¿™æ ·ä¼šå¢žåŠ UNKçš„æ¯”ä¾‹ã€‚
                é»˜è®¤googleçš„tokenizeræ˜¯greedy_sharp=False çš„å½¢å¼ã€‚
                å¦‚æžœgreedy_sharp æ˜¯trueï¼Œåˆ™ä¼šå…ˆçœ‹ "##x" æ˜¯åœ¨è¯è¡¨é‡Œï¼Œå¦‚æžœä¸åœ¨ï¼Œä¼šçœ‹ "x" æ˜¯å¦åœ¨è¯è¡¨é‡Œã€‚
            is_trace: æ˜¯å¦åœ¨traceçš„æ¨¡å¼ä¸‹ï¼Œmatxæœ‰ä¸€äº›warpçš„é€»è¾‘
        """
        vocab_file = get_local_vocab_file(vocab_file)
        self.is_trace = is_trace
        self.add_title = add_title
        self.max_seq_len = max_seq_len
        self.max_seq_len = max_seq_len
        self.thread_num = thread_num

        # text tokenizer
        do_wordpiece = not greedy_sharp
        word_piece_tokenizer = matx_text.WordPieceTokenizerOp(location=vocab_file,
                                                              do_wordpiece=do_wordpiece,
                                                              do_lower_case=do_lower_case)

        # å¦‚æžœæ˜¯ trace æ¨¡å¼ä¸‹ï¼Œåˆå§‹åŒ–æ—¶éœ€è¦ `matx.script` ä¸€ä¸‹
        if self.is_trace:
            self.task_manager = matx.script(TaskManager)(pool_size=self.thread_num,
                                                         use_lockfree_pool=True)
            self.matx_bert_tokenizer = matx.script(BertTokenizer)(tokenizer=word_piece_tokenizer,
                                                                  do_lower_case=do_lower_case,
                                                                  tokenize_emoji=tokenize_emoji,
                                                                  task_manager=self.task_manager)
            # domain æ‹¼æŽ¥ op
            self.multi_domain_concat_builder = matx.script(
                MultiDomainConcatBuilder)(max_seq_len=max_seq_len)

            # å°† batch_inputs_tokensè½¬ä¸ºinput_ids_tensor, segment_ids_tensorå’Œmask_ids_tensor
            self.build_input_builder = matx.script(BertInputsBuilder)(
                max_seq_len=max_seq_len, vocab_file=vocab_file)
        else:
            self.matx_bert_tokenizer = BertTokenizer(tokenizer=word_piece_tokenizer,
                                                     do_lower_case=do_lower_case,
                                                     tokenize_emoji=tokenize_emoji)
            self.multi_domain_concat_builder = MultiDomainConcatBuilder(
                max_seq_len=max_seq_len)
            self.build_input_builder = BertInputsBuilder(
                max_seq_len=max_seq_len, vocab_file=vocab_file)

    def __call__(self, *args, **kwargs):
        if self.is_trace:
            return self.trace_process(*args, **kwargs)
        else:
            return self.train_process(*args, **kwargs)

    def train_process(self,
                      queries: List[str],
                      titles: List[str] = []):
        """
        è®­ç»ƒçš„å¤„ç†è¿‡ç¨‹
        è¿™é‡Œå†™æ­»äº†å¿…é¡»æœ‰è¿™ä¹ˆäº›ä¸ªfields
        """

        batch_output_tensor: Dict[str, torch.Tensor] = {}
        # process query
        query_tokens = self.matx_bert_tokenizer(queries)
        query_input_tokens, query_segment_ids = self.multi_domain_concat_builder(
            [query_tokens], [0], [16])
        query_input_tensor, query_segment_tensor, query_mask_tensor = self.build_input_builder(
            query_input_tokens, query_segment_ids)
        batch_output_tensor["query_input_ids"] = torch.tensor(
            query_input_tensor.asnumpy())
        batch_output_tensor["query_segment_ids"] = torch.tensor(
            query_segment_tensor.asnumpy())
        batch_output_tensor["query_input_mask"] = torch.tensor(
            query_mask_tensor.asnumpy())

        if self.add_title and len(titles) > 0:
            titles_tokens = self.matx_bert_tokenizer(titles)
            titles_input_tokens, titles_segment_ids = self.multi_domain_concat_builder(
                [titles_tokens], [0], [self.max_seq_len])
            titles_input_tensor, titles_segment_tensor, titles_mask_tensor = self.build_input_builder(
                titles_input_tokens, titles_segment_ids)
            batch_output_tensor["title_input_ids"] = torch.tensor(
                titles_input_tensor.asnumpy())
            batch_output_tensor["title_segment_ids"] = torch.tensor(
                titles_segment_tensor.asnumpy())
            batch_output_tensor["title_input_mask"] = torch.tensor(
                titles_mask_tensor.asnumpy())

        return batch_output_tensor

    def trace_process(self,
                      queries: List[str],
                      titles: List[str]):
        """
        Traceçš„æ•´ä¸ªè¿‡ç¨‹
        """

        # tokenizer
        query_tokens = self.matx_bert_tokenizer(queries)
        titles_tokens = self.matx_bert_tokenizer(titles)

        # multi domain concat
        query_input_tokens, query_segment_ids = self.multi_domain_concat_builder(
            [query_tokens], [0], [self.max_seq_len])
        query_input_tensor, query_segment_tensor, query_mask_tensor = self.build_input_builder(
            query_input_tokens, query_segment_ids)

        titles_input_tokens, titles_segment_ids = self.multi_domain_concat_builder(
            [titles_tokens], [0], [self.max_seq_len])
        titles_input_tensor, titles_segment_tensor, titles_mask_tensor = self.build_input_builder(
            titles_input_tokens, titles_segment_ids)

        return query_input_tensor, query_segment_tensor, query_mask_tensor, titles_input_tensor, titles_segment_tensor, titles_mask_tensor
